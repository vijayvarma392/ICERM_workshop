{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a 1-dimensional surrogate\n",
    "\n",
    "ICERM Workshop: Statistical Methods for the Detection, Classification, and Inference of Relativistic Objects\n",
    "\n",
    "Date: 11/18/2020\n",
    "\n",
    "Author: Scott Field\n",
    "\n",
    "**Note**: In addition to the usual Python libraries, you will need rompy. And optionally lal and lalsimulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on packages....\n",
    "\n",
    "1) Don't have rompy? Get it now from https://bitbucket.org/chadgalley/rompy (Python 2)\n",
    "\n",
    "```bash\n",
    ">>> git clone git@bitbucket.org:chadgalley/rompy.git\n",
    "```\n",
    "\n",
    "and since its pure-python, you can install it by simply adding rompy to your PYTHONPATH variable or, from this notebook,\n",
    "\n",
    "```\n",
    "import sys\n",
    "sys.path.append(\"/path/to/rompy/\")\n",
    "```\n",
    "\n",
    "Or (new way!) install forked-rompy from pip\n",
    "\n",
    "\n",
    "```bash\n",
    ">>> pip install forked-rompy\n",
    "```\n",
    "\n",
    "\n",
    "2) Don't have lal and lalsimulation? For the purposes of this notebook, you can instead use pre-made data (relevant commands are described later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session goal...\n",
    "\n",
    "This notebook describes how to build a simple 1-dimensional surrogate model of your favorite 1-dimensional gravitational-wave model:\n",
    "\n",
    "\\begin{align*}\n",
    "h(t, \\theta, \\phi; q) & = h_+(t, \\theta, \\phi; q) - \\mathrm{i} h_x(t, \\theta, \\phi; q) \\\\\n",
    "& = \\sum_{\\ell=2}^{\\infty} \\sum_{m=-\\ell}^{\\ell} h^{\\ell m}(t;q) {}_{-2}Y_{\\ell m} \\left(\\theta, \\phi \\right) \\, ,\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\theta$ and $\\phi$ are angles for the direction of propagation away from the source, $q$ is the mass ratio and  ${}_{-2}Y_{\\ell m}$ are the harmonic functions.\n",
    "\n",
    "We will build a surrogte for the (2,2)-mode\n",
    "\n",
    "$$h^{22}(t;q)$$\n",
    "\n",
    "\n",
    "Our building strategy will be completely new! We will mix different methods according to:\n",
    "\n",
    "* Align waveforms in time, but not phase\n",
    "* SVD basis\n",
    "* Empirical interpolant representation\n",
    "* Splines to approximate the real/imaginary parts of h(T;q) for T fixed and q variying\n",
    "\n",
    "While each part has been reported in the litterature (being a mixture of methods considered in https://arxiv.org/abs/1402.4146 and https://arxiv.org/abs/1308.3565), this combination hasn't been considered before. This illustrates the flexability of surrogate building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual suspects\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: evaluating the model\n",
    "\n",
    "In this first lesson we check that lalsimulation is installed, and you are able to evaluate the EOBNRv2 model.\n",
    "\n",
    "If you do not have lalsimulation installed, you can skip this 0th step altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lalsimulation as LS\n",
    "import lal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simplified interface to the EOB model\n",
    "\n",
    "def Mq_to_m1m2(M, q):\n",
    "  \"\"\"Convert total mass, mass ratio pair to m1, m2\"\"\"\n",
    "  m2 = M/(1.+q)\n",
    "  m1 = M-m2\n",
    "  return m1, m2\n",
    "\n",
    "def EOBNRv2_LAL_modes(Mtot=80.0,q=1.0,dt=1.0/2048.,fmin=10.0,Dist=1.0,ell=2,emm=2):\n",
    "  \"\"\" Simplified inferface to EOBNRv2 mode.\n",
    "\n",
    "      INPUT\n",
    "      =====\n",
    "      Dist -- distance in megaparsecs.\n",
    "      Mtot -- total mass in solar masses\"\"\"\n",
    "\n",
    "  Dist = Dist * 1e6 *lal.PC_SI\n",
    "  Mtot = Mtot * lal.MSUN_SI\n",
    "\n",
    "  M1,M2  = Mq_to_m1m2(Mtot,q)\n",
    "\n",
    "  h      = LS.SimInspiralChooseTDMode(deltaT=dt,m1=M1,m2=M2,\n",
    "                                      f_min=fmin,f_ref=0.0,r=Dist,\n",
    "                                      lambda1=0.0,lambda2=0.0,waveFlags=None,\n",
    "                                      nonGRparams=None,amplitudeO=0,\n",
    "                                      phaseO=7,l=ell,m=emm,approximant=LS.EOBNRv2)\n",
    "\n",
    "  times = np.arange(np.size(h.data.data))*h.deltaT\n",
    "\n",
    "  return times, h.data.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, h = EOBNRv2_LAL_modes(80.0,1.0,1.0/2048.0,10.0,1.0,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,np.real(h),'r',label = \"real\")\n",
    "plt.plot(t,np.imag(h),'k--',label = \"imag\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Building the training set\n",
    "\n",
    "\n",
    "\n",
    "There are potentially many reasonable ways to construct a training set defined on the training region\n",
    "\n",
    "$$q \\in [1, 2]$$\n",
    "\n",
    "and we will use the easiest of all: uniformly spaced grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings that control the training set\n",
    "dt = 1.0/2048.\n",
    "train_samples = 100 # more waveforms, better models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_set_generator(N,verbose=False):\n",
    "    \"\"\"Generate N training samples from q in [1,2]\"\"\"\n",
    "    qs = np.linspace(1.0,2.0,N)\n",
    "    training_data = []\n",
    "    for q in qs:\n",
    "        t,h = EOBNRv2_LAL_modes(Mtot=80.0,q=q,dt=dt,fmin=10.0,Dist=1.0,ell=2,emm=2)\n",
    "        training_data.append(h)\n",
    "        if verbose:\n",
    "            print('length of h is %i'%len(h))\n",
    "    return qs, training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating training data\n",
    "qs, training_data = training_set_generator(train_samples, verbose=True)\n",
    "\n",
    "# if you don't have lal, instead do:\n",
    "#qs = np.loadtxt(\"data/1D_EOBNRv2/q_values_training.txt\")\n",
    "#training_data = np.load(\"data/1D_EOBNRv2/training_data.npy\",allow_pickle=True,encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude I: preparing waveform data -- common durations\n",
    "\n",
    "For surrogate modeling to work, we need all of the waveforms to be of the same duration/length. From whats shown above, this is not the case. We need to modify our waveform data to fix this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_time_grid(training_data):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    =====\n",
    "    training_data: set of training waveforms \n",
    "    \n",
    "    OUTPUT\n",
    "    ======\n",
    "    training data as a numpy array, padding with zeros as \n",
    "    necessary such that all waveforms are of the same length\"\"\"\n",
    "    \n",
    "    longest_waveform = 0\n",
    "    for h in training_data:\n",
    "        length = len(h)\n",
    "        if length > longest_waveform:\n",
    "            longest_waveform = length\n",
    "            \n",
    "    print(\"longest waveform size = %i\"%longest_waveform)\n",
    "        \n",
    "    padded_training_data = []\n",
    "    for h in training_data:\n",
    "        nZeros = longest_waveform - len(h)\n",
    "        h_pad = np.append(h, np.zeros(nZeros))\n",
    "        padded_training_data.append(h_pad)\n",
    "        \n",
    "    times = np.arange(longest_waveform)*dt\n",
    "    \n",
    "    padded_training_data = np.vstack(padded_training_data).transpose()\n",
    "    \n",
    "    return times, padded_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times, training_data = common_time_grid(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(times,np.abs(training_data))\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(times,np.abs(training_data))\n",
    "plt.xlim([3,4.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude II: preparing waveform data -- temporal alignment\n",
    "\n",
    "For surrogate modeling to work, the model should change as little as possible as the parameter is varied. Unfortunately, thats not the case here. A simulation-dependent time-shift will align the peaks. \n",
    "\n",
    "Lets align all of the waveform peaks, using the shortest waveform as the reference one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak(t, h):\n",
    "  \"\"\"Get argument and values of t and h at maximum value of |h| on a discrete grid. \"\"\"\n",
    "  arg = np.argmax(np.abs(h))\n",
    "  return [arg, t[arg], h[arg]]\n",
    "\n",
    "def get_peaks(t,training_set):\n",
    "    \"\"\" Find the index of each waveform's peak in the entire training set. \"\"\"\n",
    "    time_peak_arg = []\n",
    "    for i in range(train_samples):\n",
    "        [arg, t_peak, h_peak] = get_peak(times,training_set[:,i]) # i^th training sample\n",
    "        time_peak_arg.append(arg)\n",
    "        print(\"Waveform %i with t_peak = %f\"%(i,t_peak))\n",
    "    print(time_peak_arg)\n",
    "    return time_peak_arg\n",
    "\n",
    "def align_peaks(times, training_set):\n",
    "    \"\"\" Peak align a set of waveforms. The shortest waveform is used as the reference\n",
    "    one.\"\"\"\n",
    "    \n",
    "    time_peak_arg = get_peaks(times,training_set)\n",
    "    \n",
    "    min_arg = min(time_peak_arg)\n",
    "    aligned_training_set=[]\n",
    "    for i in range(train_samples):\n",
    "        offset = time_peak_arg[i] - min_arg\n",
    "        print(\"offset value of %i\"%offset)\n",
    "        h_aligned = training_set[offset:,i]\n",
    "        aligned_training_set.append(h_aligned)\n",
    "        \n",
    "    t, training_data_aligned = common_time_grid(aligned_training_set)\n",
    "    return training_data_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_aligned = align_peaks(times, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(times,np.abs(training_data_aligned))\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(times,np.abs(training_data_aligned),'*')\n",
    "plt.xlim([3.565,3.585])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check (its hard to tell from figure that these peaks are aligned)\n",
    "get_peaks(times,training_data_aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Building a basis\n",
    "\n",
    "The next step is to compress the model by finding a basis. The greedy algorithm and singular value decomposition (SVD) are two popular approaches. Here we will use the SVD, which is already coded up in numpy. \n",
    "\n",
    "The greedy (reduced basis) algorithm is a often used alternative\n",
    "\n",
    "### background on the SVD\n",
    "\n",
    "You can think of a SVD as an algorithm which identifies the \"most important\" columns of a matrix. A column is \"important\" if the other columns of the matrix can be written as a linear combination of the important ones. \n",
    "\n",
    "Suppose you have a 10000-by-100 matrix, ${\\bf A}$, and you discover that all 100 columns can be written as a linear combination of just 2 columns. That would be great -- you could compress the matrix by storing just 2 columns! This is what the SVD provides, and how it can  be used to approximate a matrix ${\\bf A}$. \n",
    "\n",
    "Since the training set of waveforms is really just a matrix, we can use the SVD to compress the training set. \n",
    "\n",
    "For our puproses, just keep in mind that the most important columns (the basis) are columns of u and the $i^{th}$ singluar values, $s_i$, assigns a weight to how important the $i^{th}$ column is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets check the claim that peak-aligned training sets are easier to approximate.\n",
    "# From the above discussion, faster decaying singular values means the matrix/model is easier\n",
    "# to compress\n",
    "u, s_no_align, vh = np.linalg.svd(training_data, full_matrices=False)\n",
    "u, s, vh          = np.linalg.svd(training_data_aligned, full_matrices=False)\n",
    "plt.semilogy(range(train_samples),s,'r*',label='align')\n",
    "plt.semilogy(range(train_samples),s_no_align,'b+',label='no align')\n",
    "plt.ylabel('singluar value')\n",
    "plt.xlabel('singluar value index k')\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top 4 singular vectors -- these are the 4 most\n",
    "# important features according to the SVD\n",
    "# if you had to use only 4 columns to represent the all training set waveforms, \n",
    "# these would be the 4 to use. \n",
    "for i in range(4):\n",
    "    plt.figure(i)\n",
    "    plt.plot(times,u[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude I:  a waveform-compression tool\n",
    "\n",
    "We can immediately use the first $n$ basis to compute \n",
    "\n",
    "$$h(t;q) \\approx \\sum_{i=1}^n  c_i (q) u_i(t)$$\n",
    "\n",
    "\n",
    "where $u_i(t)$ is the $i^{th}$ basis vector and $c_i (q)$ are the coefficients of the representation of $h(t;q)$ in the approximation space span$\\{u_i\\}_{i=1}^n$. Since $u_i$ are orthonormal, the coefficient $c_i (q)$ is found by takind the inner produce of $h(t;q)$ with $u_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on how many column vectors to use as the basis\n",
    "basis_size = 7 # More vectors -> more accuracy but also more computational cost \n",
    "basis_set  = u[:,0:basis_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a waveform to approximate \n",
    "h_i = training_data_aligned[:,0]\n",
    "\n",
    "# compute its representation in the linear space defined by the span of the basis set\n",
    "proj_coeffs = np.dot(basis_set.transpose(), h_i.conjugate()) \n",
    "h_approx = np.dot(basis_set, proj_coeffs.transpose())\n",
    "h_approx = h_approx.transpose()\n",
    "\n",
    "# plot the original waveform, its compressed version, and the error\n",
    "plt.figure(1)\n",
    "plt.plot(times,h_i,'blue')\n",
    "plt.plot(times,h_approx,'r--')\n",
    "\n",
    "plt.figure(2)\n",
    "wave_err = np.abs(h_i - h_approx)\n",
    "plt.semilogy(times,wave_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short exercise 1\n",
    "\n",
    "Explore the relationshiop between the singular value spectrum, s, and the waveform error.\n",
    "\n",
    "Task: Select some waveform from your training set to approximate. Compute the approximation error vs # of SVD basis. Plot this error along with the singular values. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a waveform to approximate \n",
    "h_i = training_data_aligned[:,0]\n",
    "\n",
    "errors = []\n",
    "bss = range(1,100)\n",
    "for bs in bss:\n",
    "    basis_set_local  = u[:,0:bs]\n",
    "\n",
    "    # compute its representation in the linear space defined by the span of the basis set\n",
    "    proj_coeffs = np.dot(basis_set_local.transpose(), h_i.conjugate()) \n",
    "    h_approx = np.dot(basis_set_local, proj_coeffs.transpose())\n",
    "    h_approx = h_approx.transpose()\n",
    "    \n",
    "    wave_err = np.abs(h_i - h_approx)\n",
    "    errors.append(np.sum(wave_err))\n",
    "    \n",
    "# plot the original waveform, its compressed version, and the error\n",
    "plt.semilogy(bss,errors,label='wave error')\n",
    "plt.semilogy(range(train_samples),s,'r*',label='singular values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: An empirical interpolant representation \n",
    "\n",
    "As an alternative to projecting onto the basis set, an empirical interpolant can be used. The main idea is to trade the $n$ peices of information, the projection coefficients $c_i$, for direct waveform evaluations.\n",
    "\n",
    "Given $n$ basis, there (usually) exists $n$ times $\\{T_i\\}_{i=1}^n$ for which the set of numbers\n",
    "$$\\{ c_i (q) \\}_{i=1}^n \\Longleftrightarrow \\{ h(T_i;q) \\}_{i=1}^n$$\n",
    "contains equivalent information in the sense that \n",
    "\n",
    "$$ h (t;q) \\approx \\sum_{i=1}^n  c_i (q) u_i(t) \\approx \\sum_{i}^n h(T_i;q) B_i(t)$$\n",
    "\n",
    "where $B_i(t)$ are just linear combinations of $u_i(t)$ (ie the approximation space, the span of the basis, is unchanged). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RomPy. if its not in your PYTHONPATH, add it now\n",
    "import sys\n",
    "sys.path.append('/home/balzani57/Repo/GitRepos/Codes')\n",
    "import rompy as rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eim = rp.EmpiricalInterpolant(basis_set.transpose(), verbose=True) # Note the transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most important pieces of information are the interpolation times T_i and the basis B_i\n",
    "eim_indicies = eim.indices\n",
    "B = eim.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what time the empirical inteprolation method has discovered as most important:\n",
    "T_eim = times[eim_indicies]\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(t,np.real(h),'k')\n",
    "plt.plot(T_eim,np.zeros_like(T_eim),'r*')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(t,np.real(h),'k')\n",
    "plt.plot(T_eim,np.zeros_like(T_eim),'r*')\n",
    "plt.xlim([3.4,3.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claim: To compute the waveform $h(t;q)$ at any time, we only need to know the waveform's value at the times $T_i$. \n",
    "\n",
    "Lets check this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a waveform to approximate \n",
    "h_i = training_data_aligned[:,0]\n",
    "\n",
    "# compute its empirical interpolant representation \n",
    "h_eim =  h_i[eim_indicies]\n",
    "h_approx = np.dot(B.transpose(), h_eim)\n",
    "\n",
    "# plot the original waveform, its empirically interpolated version, and the error\n",
    "plt.figure(1)\n",
    "plt.plot(times,h_i,'blue')\n",
    "plt.plot(times,h_approx,'r--')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.semilogy(times,np.abs(h_i - h_approx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short exercise 2\n",
    "\n",
    "Explain why the error somtimes drops to numerical zero. Provide numerical evidence to justify your answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The empirical interpolant interpolates the data. So the errors will be zero at the EIM nodes\n",
    "\n",
    "plt.figure(2)\n",
    "plt.semilogy(times,np.abs(h_i - h_approx))\n",
    "plt.semilogy(T_eim,np.ones_like(T_eim)*1.e-30,'r*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Parametric fits\n",
    "\n",
    "We are almost ready to complete our surrogate model! If we could evaluate the few functions $h(T_i;q)$ at any q, then we would be done. So what remains is finding a way to predict their values.\n",
    "\n",
    "Here one often uses interpolation (e.g. polynomials, splines, etc), fits/regressions, or, more recently, Gaussian process regression. One could imagine many other resonable choices too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, view the data we need to fit\n",
    "h_training_eim = training_data_aligned[eim_indicies,:]\n",
    "\n",
    "for counter, i in enumerate(eim_indicies):\n",
    "    plt.figure(counter)\n",
    "    plt.title(\"eim index %i at t[i] = %f\"%(i,times[i]))\n",
    "    plt.plot(qs,np.real(h_training_eim[counter,:]),'r',label='real' )\n",
    "    plt.plot(qs,np.imag(h_training_eim[counter,:]),'k--',label='imag' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: dealing with hard-to-aproximate functions\n",
    "\n",
    "Evidently, these functions have a significant amount of struture in $q$. There are a few strategies for dealing with this:\n",
    "\n",
    "1) One could look for adventageous decompositions of the data, for example,\n",
    "\n",
    "$$h(T_i;q) = A(T_i;q) \\exp\\left(-\\mathrm{i} \\phi(T_i;q) \\right)$$\n",
    "\n",
    "since the amplitude, $A$, and phase, $\\phi$ are expected to be \"borring\" functions of $q$\n",
    "\n",
    "2) One could align the phases at some reference time. This would happen early in the surrogate building process, say step 1 after temporal alignment\n",
    "\n",
    "3) One could brute force it, by using splines with dense grids\n",
    "\n",
    "... we will follow 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import splrep, splev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first show how this works on a data piece \n",
    "eim_indx = 1\n",
    "h_eim_real_spline = splrep(qs, np.real(h_training_eim[eim_indx,:]),k=2) # degree 2\n",
    "h_eim_imag_spline = splrep(qs, np.imag(h_training_eim[eim_indx,:]),k=2) # degree 2\n",
    "\n",
    "q_dense = np.linspace(min(qs),max(qs),300)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(qs,np.real(h_training_eim[eim_indx,:]),'k',label ='real, data')\n",
    "plt.plot(q_dense,splev(q_dense,h_eim_real_spline),'r--',label ='real, spline')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(qs,np.imag(h_training_eim[eim_indx,:]),'k',label ='imag, data')\n",
    "plt.plot(q_dense,splev(q_dense,h_eim_imag_spline),'r--',label ='imag, spline')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now find the spline interpolant for all of the coefficients \n",
    "h_eim_real_spline = [splrep(qs, np.real(h_training_eim[i,:]),k=2) for i in range(len(eim_indicies))]\n",
    "h_eim_imag_spline = [splrep(qs, np.imag(h_training_eim[i,:]),k=2) for i in range(len(eim_indicies))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full surrogate can now be evaluated by evaluating the splines, and using the \n",
    "# empirical interpolation representation\n",
    "\n",
    "q=1.2 # mass ratio value to predict now model for\n",
    "\n",
    "h_eim = np.array([splev(q, h_eim_real_spline[j])  \\\n",
    "             + 1.0j*splev(q,h_eim_imag_spline[j]) for j in range(len(eim_indicies))])\n",
    "h_approx = np.dot(B.transpose(), h_eim)\n",
    "\n",
    "# plot the original waveform, its empirically interpolated version, and the error\n",
    "plt.figure(1)\n",
    "plt.plot(times,np.real(h_approx),'k')\n",
    "plt.plot(times,np.imag(h_approx),'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Full surrogate\n",
    "\n",
    "We're basically done. \n",
    "\n",
    "Lets package up all the data to make it more user friendly. Then sanity test the model by comparing against the training data. Show its faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate(q):\n",
    "    h_eim = np.array([splev(q, h_eim_real_spline[j])  \\\n",
    "             + 1.0j*splev(q,h_eim_imag_spline[j]) for j in range(len(eim_indicies))])\n",
    "    h_approx = np.dot(B.transpose(), h_eim)\n",
    "    return h_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_surr = surrogate(q=1.5)\n",
    "plt.figure(1)\n",
    "plt.plot(times,np.real(h_surr),'k')\n",
    "plt.plot(times,np.imag(h_surr),'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing experiment -- EOB model from LAL\n",
    "%time qs, training_data = training_set_generator(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing experiment -- EOB surrogate model\n",
    "%time surrogate_evals = np.array([surrogate(q) for q in qs]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy as relative error in max norm\n",
    "# here we compare the model against the training data\n",
    "h_error = np.abs(training_data_aligned - surrogate_evals)\n",
    "h_inf =  h_error.max(axis=0) / np.abs(training_data_aligned).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qs,h_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "The surrogate model seems to be working well. Further directions to explore include...\n",
    "\n",
    "* How do the errors compare against testing waveforms not used to train?\n",
    "* What is the dominant source of model error? Do the errors get smaller as... the number of basis used is increased? The training set density is increased? Different order splines are used? The sampling rate dt is decreased? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
